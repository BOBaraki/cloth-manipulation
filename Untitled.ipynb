{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "from PIL import Image\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook, tqdm, trange\n",
    "# to print multiple outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "from linformer import Linformer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/tzortzis/cloth_data/'\n",
    "writer = SummaryWriter(\"runs/clothes-3-eren2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>cloth_state</th>\n",
       "      <th>gripper_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>image_0</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>image_1</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>image_2</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>image_3</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>image_4</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        state   cloth_state gripper_state\n",
       "0           0  image_0  approaching  straightened          free\n",
       "1           1  image_1  approaching  straightened          free\n",
       "2           2  image_2  approaching  straightened          free\n",
       "3           3  image_3  approaching  straightened          free\n",
       "4           4  image_4  approaching  straightened          free"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>cloth_state</th>\n",
       "      <th>gripper_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>image_0</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>image_1</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>image_2</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>image_3</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>image_4</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        state   cloth_state gripper_state\n",
       "0           0  image_0  approaching  straightened          free\n",
       "1           1  image_1  approaching  straightened          free\n",
       "2           2  image_2  approaching  straightened          free\n",
       "3           3  image_3  approaching  straightened          free\n",
       "4           4  image_4  approaching  straightened          free"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_df = pd.read_csv(os.path.join(data_path,'train/data.csv'))\n",
    "all_labels_df_test = pd.read_csv(os.path.join(data_path,'test/data.csv'))\n",
    "all_labels_df.head()\n",
    "all_labels_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = all_labels_df.state.unique()\n",
    "state2idx = dict((state,idx) for idx,state in enumerate(state))\n",
    "idx2state = dict((idx,state) for idx,state in enumerate(state))\n",
    "\n",
    "state_test = all_labels_df_test.state.unique()\n",
    "state2idx_test = dict((state_test,idx) for idx,state_test in enumerate(state_test))\n",
    "idx2state_test = dict((idx,state_test) for idx,state_test in enumerate(state_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>cloth_state</th>\n",
       "      <th>gripper_state</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>image_0</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>image_1</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>image_2</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>image_3</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>image_4</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        state   cloth_state gripper_state  label_idx\n",
       "0           0  image_0  approaching  straightened          free          0\n",
       "1           1  image_1  approaching  straightened          free          0\n",
       "2           2  image_2  approaching  straightened          free          0\n",
       "3           3  image_3  approaching  straightened          free          0\n",
       "4           4  image_4  approaching  straightened          free          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>cloth_state</th>\n",
       "      <th>gripper_state</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>image_0</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>image_1</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>image_2</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>image_3</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>image_4</td>\n",
       "      <td>approaching</td>\n",
       "      <td>straightened</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id        state   cloth_state gripper_state  label_idx\n",
       "0           0  image_0  approaching  straightened          free          0\n",
       "1           1  image_1  approaching  straightened          free          0\n",
       "2           2  image_2  approaching  straightened          free          0\n",
       "3           3  image_3  approaching  straightened          free          0\n",
       "4           4  image_4  approaching  straightened          free          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_df['label_idx'] = [state2idx[b] for b in all_labels_df.state]\n",
    "all_labels_df.head()\n",
    "all_labels_df_test['label_idx'] = [state2idx_test[b] for b in all_labels_df_test.state]\n",
    "all_labels_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8383"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1695"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels_df['label_idx'])\n",
    "len(all_labels_df_test['label_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state)\n",
    "len(state_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, labels_df, img_path, transform=None):\n",
    "        self.labels_df = labels_df\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.join(self.img_path, self.labels_df.id[idx]) + '.png'\n",
    "        img = Image.open(image_name)\n",
    "        label = self.labels_df.label_idx[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CustomDataset(all_labels_df, os.path.join(data_path, 'train/RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=256x256 at 0x7F9F3D9DDC88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[30][0] # :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "bs = 32 #If you get a cuda out of memory error restart the kernel and change it into 32 or 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.RandomResizedCrop(input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])#these vectors is the mean and the std from the statistics in imagenet. They are always the same as far as I can recall\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=0)\n",
    "train_split_idx, val_split_idx = next(iter(stratified_split.split(all_labels_df.id, all_labels_df.state)))\n",
    "train_df = all_labels_df.iloc[train_split_idx].reset_index()\n",
    "val_df = all_labels_df.iloc[val_split_idx].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6706, 1677, 1695)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df), len(all_labels_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, os.path.join(data_path,'train/RGB'), transform=data_transforms['train'])\n",
    "val_dataset = CustomDataset(val_df, os.path.join(data_path,'train/RGB'), transform=data_transforms['val'])\n",
    "test_dataset = CustomDataset(all_labels_df_test, os.path.join(data_path,'test/RGB'), transform=data_transforms['val'])\n",
    "image_dataset = {'train':train_dataset, 'val':val_dataset, 'test':test_dataset}\n",
    "dataset_names = ['train', 'val','test']\n",
    "\n",
    "image_dataloader = {x:DataLoader(image_dataset[x],batch_size=bs,shuffle=True,num_workers=6) for x in dataset_names}\n",
    "dataset_sizes = {x:len(image_dataset[x]) for x in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6706, 'val': 1677, 'test': 1695}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, scheduler, dataset_sizes, image_dataloader, dataset_names, num_epochs=25):\n",
    "    start_time = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    \n",
    "    # Each epoch has a training and validation phase\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {0}/{1}'.format(epoch+1, num_epochs), end='\\t')\n",
    "\n",
    "        for phase in dataset_names:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train() # Set model to training mode\n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for data in image_dataloader[phase]:\n",
    "                inps, labels = data\n",
    "                inps = inps.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inps)\n",
    "                    _,preds = torch.max(outputs.data,1)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inps.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            writer.add_scalars(phase, {'loss':epoch_loss,\n",
    "                                                   'accuracy':epoch_acc},epoch + 1)\n",
    "            writer.close()\n",
    "            \n",
    "            # deep copy model with the best weights\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, best_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tzortzis/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "model_ft = torch.hub.load('pytorch/vision:v0.6.0', 'resnext50_32x4d', pretrained=True)\n",
    "# model_ft = ViT('B_16', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True    # By doing this I am keeping the parameters of the feature layers frozen so they won't update\n",
    "\n",
    "num_fc_ftr = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_fc_ftr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params':model_ft.fc.parameters()}\n",
    "# ], lr=0.001)\n",
    "optimizer = optim.SGD(model_ft.fc.parameters(), lr=0.1)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.01, last_epoch=-1)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f8df73e1667f272f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f8df73e1667f272f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\ttrain Loss: 4.0756 Acc: 0.6162\n",
      "val Loss: 0.4815 Acc: 0.8778\n",
      "test Loss: 0.4390 Acc: 0.8873\n",
      "\n",
      "Epoch 2/100\ttrain Loss: 1.6136 Acc: 0.7261\n",
      "val Loss: 2.9688 Acc: 0.6291\n",
      "test Loss: 2.8403 Acc: 0.6301\n",
      "\n",
      "Epoch 3/100\ttrain Loss: 1.2756 Acc: 0.7630\n",
      "val Loss: 0.3631 Acc: 0.9147\n",
      "test Loss: 0.3161 Acc: 0.9180\n",
      "\n",
      "Epoch 4/100\ttrain Loss: 1.0305 Acc: 0.7802\n",
      "val Loss: 0.4608 Acc: 0.8897\n",
      "test Loss: 0.4176 Acc: 0.8985\n",
      "\n",
      "Epoch 5/100\ttrain Loss: 0.6782 Acc: 0.8148\n",
      "val Loss: 0.7157 Acc: 0.8473\n",
      "test Loss: 0.6676 Acc: 0.8496\n",
      "\n",
      "Epoch 6/100\ttrain Loss: 0.5581 Acc: 0.8248\n",
      "val Loss: 0.5504 Acc: 0.8682\n",
      "test Loss: 0.4972 Acc: 0.8684\n",
      "\n",
      "Epoch 7/100\ttrain Loss: 0.4985 Acc: 0.8215\n",
      "val Loss: 0.2593 Acc: 0.9171\n",
      "test Loss: 0.2300 Acc: 0.9233\n",
      "\n",
      "Epoch 8/100\ttrain Loss: 0.4852 Acc: 0.8281\n",
      "val Loss: 0.3762 Acc: 0.8885\n",
      "test Loss: 0.3451 Acc: 0.8985\n",
      "\n",
      "Epoch 9/100\ttrain Loss: 0.4513 Acc: 0.8384\n",
      "val Loss: 0.2264 Acc: 0.9237\n",
      "test Loss: 0.2008 Acc: 0.9263\n",
      "\n",
      "Epoch 10/100\ttrain Loss: 2.2889 Acc: 0.6921\n",
      "val Loss: 1.9270 Acc: 0.7352\n",
      "test Loss: 1.8763 Acc: 0.7298\n",
      "\n",
      "Epoch 11/100\ttrain Loss: 1.2009 Acc: 0.7853\n",
      "val Loss: 0.4575 Acc: 0.8945\n",
      "test Loss: 0.4180 Acc: 0.9056\n",
      "\n",
      "Epoch 12/100\ttrain Loss: 1.3130 Acc: 0.7696\n",
      "val Loss: 2.1501 Acc: 0.7132\n",
      "test Loss: 2.0283 Acc: 0.7127\n",
      "\n",
      "Epoch 13/100\ttrain Loss: 1.1591 Acc: 0.7908\n",
      "val Loss: 0.3190 Acc: 0.9225\n",
      "test Loss: 0.2697 Acc: 0.9298\n",
      "\n",
      "Epoch 14/100\ttrain Loss: 1.1370 Acc: 0.7896\n",
      "val Loss: 0.8374 Acc: 0.8509\n",
      "test Loss: 0.7588 Acc: 0.8484\n",
      "\n",
      "Epoch 15/100\ttrain Loss: 1.1252 Acc: 0.7756\n",
      "val Loss: 0.3822 Acc: 0.9153\n",
      "test Loss: 0.3437 Acc: 0.9091\n",
      "\n",
      "Epoch 16/100\ttrain Loss: 1.0105 Acc: 0.7917\n",
      "val Loss: 1.0494 Acc: 0.8193\n",
      "test Loss: 0.9621 Acc: 0.8195\n",
      "\n",
      "Epoch 17/100\ttrain Loss: 0.7945 Acc: 0.8115\n",
      "val Loss: 0.6633 Acc: 0.8724\n",
      "test Loss: 0.5908 Acc: 0.8720\n",
      "\n",
      "Epoch 18/100\ttrain Loss: 0.8328 Acc: 0.8061\n",
      "val Loss: 0.2303 Acc: 0.9284\n",
      "test Loss: 0.2219 Acc: 0.9369\n",
      "\n",
      "Epoch 19/100\ttrain Loss: 0.7577 Acc: 0.8111\n",
      "val Loss: 0.2261 Acc: 0.9255\n",
      "test Loss: 0.2095 Acc: 0.9345\n",
      "\n",
      "Epoch 20/100\ttrain Loss: 0.6271 Acc: 0.8236\n",
      "val Loss: 0.1985 Acc: 0.9284\n",
      "test Loss: 0.1822 Acc: 0.9375\n",
      "\n",
      "Epoch 21/100\ttrain Loss: 0.6069 Acc: 0.8188\n",
      "val Loss: 0.2593 Acc: 0.9213\n",
      "test Loss: 0.2458 Acc: 0.9245\n",
      "\n",
      "Epoch 22/100\ttrain Loss: 0.5577 Acc: 0.8246\n",
      "val Loss: 0.1865 Acc: 0.9332\n",
      "test Loss: 0.1803 Acc: 0.9404\n",
      "\n",
      "Epoch 23/100\ttrain Loss: 0.4952 Acc: 0.8358\n",
      "val Loss: 0.2803 Acc: 0.9147\n",
      "test Loss: 0.2518 Acc: 0.9215\n",
      "\n",
      "Epoch 24/100\ttrain Loss: 0.4549 Acc: 0.8451\n",
      "val Loss: 0.2336 Acc: 0.9243\n",
      "test Loss: 0.2111 Acc: 0.9257\n",
      "\n",
      "Epoch 25/100\ttrain Loss: 0.4603 Acc: 0.8351\n",
      "val Loss: 0.1798 Acc: 0.9332\n",
      "test Loss: 0.1658 Acc: 0.9375\n",
      "\n",
      "Epoch 26/100\ttrain Loss: 0.4165 Acc: 0.8434\n",
      "val Loss: 0.1785 Acc: 0.9374\n",
      "test Loss: 0.1642 Acc: 0.9410\n",
      "\n",
      "Epoch 27/100\ttrain Loss: 0.4055 Acc: 0.8440\n",
      "val Loss: 0.1664 Acc: 0.9392\n",
      "test Loss: 0.1542 Acc: 0.9375\n",
      "\n",
      "Epoch 28/100\ttrain Loss: 0.3949 Acc: 0.8495\n",
      "val Loss: 0.1738 Acc: 0.9302\n",
      "test Loss: 0.1724 Acc: 0.9345\n",
      "\n",
      "Epoch 29/100\ttrain Loss: 0.4110 Acc: 0.8482\n",
      "val Loss: 0.1685 Acc: 0.9350\n",
      "test Loss: 0.1621 Acc: 0.9345\n",
      "\n",
      "Epoch 30/100\ttrain Loss: 2.1450 Acc: 0.7030\n",
      "val Loss: 3.1080 Acc: 0.6327\n",
      "test Loss: 3.1869 Acc: 0.6330\n",
      "\n",
      "Epoch 31/100\ttrain Loss: 1.3068 Acc: 0.7724\n",
      "val Loss: 0.2799 Acc: 0.9302\n",
      "test Loss: 0.2692 Acc: 0.9310\n",
      "\n",
      "Epoch 32/100\ttrain Loss: 1.2854 Acc: 0.7778\n",
      "val Loss: 1.0646 Acc: 0.8277\n",
      "test Loss: 0.9937 Acc: 0.8283\n",
      "\n",
      "Epoch 33/100\ttrain Loss: 1.1619 Acc: 0.7909\n",
      "val Loss: 0.3536 Acc: 0.9213\n",
      "test Loss: 0.3368 Acc: 0.9204\n",
      "\n",
      "Epoch 34/100\ttrain Loss: 1.2442 Acc: 0.7780\n",
      "val Loss: 0.2419 Acc: 0.9290\n",
      "test Loss: 0.2351 Acc: 0.9392\n",
      "\n",
      "Epoch 35/100\ttrain Loss: 1.1593 Acc: 0.7938\n",
      "val Loss: 0.5628 Acc: 0.8921\n",
      "test Loss: 0.4988 Acc: 0.8932\n",
      "\n",
      "Epoch 36/100\ttrain Loss: 1.1188 Acc: 0.7894\n",
      "val Loss: 0.2565 Acc: 0.9237\n",
      "test Loss: 0.2758 Acc: 0.9363\n",
      "\n",
      "Epoch 37/100\ttrain Loss: 1.2163 Acc: 0.7850\n",
      "val Loss: 0.2361 Acc: 0.9273\n",
      "test Loss: 0.2284 Acc: 0.9416\n",
      "\n",
      "Epoch 38/100\ttrain Loss: 1.0934 Acc: 0.7993\n",
      "val Loss: 0.2711 Acc: 0.9249\n",
      "test Loss: 0.2552 Acc: 0.9316\n",
      "\n",
      "Epoch 39/100\ttrain Loss: 0.9719 Acc: 0.8066\n",
      "val Loss: 1.9442 Acc: 0.6893\n",
      "test Loss: 1.8222 Acc: 0.6873\n",
      "\n",
      "Epoch 40/100\ttrain Loss: 0.9486 Acc: 0.8038\n",
      "val Loss: 0.4217 Acc: 0.9064\n",
      "test Loss: 0.3642 Acc: 0.9156\n",
      "\n",
      "Epoch 41/100\ttrain Loss: 0.9668 Acc: 0.8038\n",
      "val Loss: 2.2434 Acc: 0.7585\n",
      "test Loss: 2.2476 Acc: 0.7599\n",
      "\n",
      "Epoch 42/100\ttrain Loss: 0.9007 Acc: 0.8097\n",
      "val Loss: 0.6923 Acc: 0.8527\n",
      "test Loss: 0.6645 Acc: 0.8549\n",
      "\n",
      "Epoch 43/100\ttrain Loss: 0.9368 Acc: 0.8003\n",
      "val Loss: 2.5179 Acc: 0.6404\n",
      "test Loss: 2.4911 Acc: 0.6401\n",
      "\n",
      "Epoch 44/100\ttrain Loss: 0.8296 Acc: 0.8085\n",
      "val Loss: 0.2611 Acc: 0.9243\n",
      "test Loss: 0.2284 Acc: 0.9351\n",
      "\n",
      "Epoch 45/100\ttrain Loss: 0.7392 Acc: 0.8142\n",
      "val Loss: 0.5257 Acc: 0.8784\n",
      "test Loss: 0.5042 Acc: 0.8649\n",
      "\n",
      "Epoch 46/100\ttrain Loss: 0.8081 Acc: 0.8094\n",
      "val Loss: 0.2336 Acc: 0.9213\n",
      "test Loss: 0.2270 Acc: 0.9304\n",
      "\n",
      "Epoch 47/100\ttrain Loss: 0.7300 Acc: 0.8143\n",
      "val Loss: 0.2051 Acc: 0.9320\n",
      "test Loss: 0.1861 Acc: 0.9345\n",
      "\n",
      "Epoch 48/100\ttrain Loss: 0.6275 Acc: 0.8249\n",
      "val Loss: 0.1757 Acc: 0.9350\n",
      "test Loss: 0.1759 Acc: 0.9363\n",
      "\n",
      "Epoch 49/100\ttrain Loss: 0.6545 Acc: 0.8160\n",
      "val Loss: 0.8699 Acc: 0.8235\n",
      "test Loss: 0.8391 Acc: 0.8147\n",
      "\n",
      "Epoch 50/100\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tzortzis/anaconda3/envs/deep_learning/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cf9f4d4dce44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard --logdir=runs/clothes-3-eren2/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# writer.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d82e5ad39cb4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_fn, optimizer, scheduler, dataset_sizes, image_dataloader, dataset_names, num_epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=runs/clothes-3-eren2/\n",
    "model_ft, best_acc = train_model(model_ft,criterion,optimizer,scheduler, dataset_sizes, image_dataloader, dataset_names, num_epochs=100)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2dd9f00bafc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_acc' is not defined"
     ]
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_ft.state_dict(), \"/home/tzortzis/sidedomain3classes_fixed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.load_state_dict(torch.load('/home/tzortzis/sidedomain3classes_fixed.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-253996672230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mclass_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         import pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "model_ft.eval()  # it-disables-dropout\n",
    "from torch.nn import functional as F\n",
    "image_dataloader_test = {x:DataLoader(image_dataset[x],batch_size=1,shuffle=True,num_workers=6) for x in dataset_names}\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    phase = 'val'\n",
    "    predictions = []\n",
    "    preds = []\n",
    "    \n",
    "    for data in image_dataloader_test['test']:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "        inps, labels = data\n",
    "        inps = inps.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(inps)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        class_predictions = [F.softmax(output, dim=0) for output in outputs]\n",
    "        preds.append(class_predictions)\n",
    "        labels.append(predicted)\n",
    "#         import pdb\n",
    "#         pdb.set_trace()\n",
    "        if (predicted == labels):\n",
    "            predictions.append('True',)\n",
    "        else:\n",
    "            predictions.append('False')\n",
    "    \n",
    "    labels = \n",
    "    \n",
    "    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [print(predictions, all_labels_df_test['state'])\n",
    "# for i in range(predictions):\n",
    "#     print(predictions(i), all_labels_df_test['state'](i))\n",
    "# all_labels_df_test['state']\n",
    "xtra = {'pred': predictions}\n",
    "# xtra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_all_labels_df_test = all_labels_df_test.append(pd.DataFrame(xtra))\n",
    "new_all_labels_df_test = all_labels_df_test.insert(2, 'pred', predictions, allow_duplicates = False)\n",
    "new_all_labels_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_df_test.to_csv(r'/home/tzortzis/cloth_data/test/predictions_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "# font = ImageFont.load(\"arial.pil\")\n",
    "\n",
    "for i in range(len(all_labels_df_test['state'])):\n",
    "    name = \"/home/tzortzis/cloth_data/test/RGB/\" + \"image_\" +str(i) + \".png\"\n",
    "    img = Image.open(name)\n",
    "    font = ImageFont.load_default().font\n",
    "# img = Image.open(\"sample_in.jpg\")\n",
    "# draw = ImageDraw.Draw(img)\n",
    "# # font = ImageFont.truetype(<font-file>, <font-size>)\n",
    "# font = ImageFont.truetype(\"sans-serif.ttf\", 16)\n",
    "# # draw.text((x, y),\"Sample Text\",(r,g,b))\n",
    "# draw.text((0, 0),\"Sample Text\",(255,255,255),font=font)\n",
    "# img.save('sample-out.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
